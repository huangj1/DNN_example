{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation]#.reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = m // mini_batch_size # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the mnist data set\n",
    "def load_data():\n",
    "    mnist = input_data.read_data_sets('G:/MNIST_data/', one_hot = True)\n",
    "    train_data = mnist.train.images.T\n",
    "    train_label = mnist.train.labels.T\n",
    "    validation_data = mnist.validation.images.T\n",
    "    validation_label = mnist.validation.labels.T\n",
    "    test_data = mnist.test.images.T\n",
    "    test_label = mnist.test.labels.T\n",
    "    return train_data,train_label,validation_data,validation_label,test_data,test_label\n",
    "# train_data,train_label,validation_data,validation_label,test_data,test_label = load_data()\n",
    "# print('train:',train_label.shape)\n",
    "# print('validation:',validation_data.shape)\n",
    "# print('test:',test_data.shape)\n",
    "\n",
    "def create_placeholders(n_x, n_y):\n",
    "    ''' create the placeholders for the tensorflow session.\n",
    "    \n",
    "    arguments:\n",
    "    n_x -- scalar, size of an image vector (28*28 = 784)\n",
    "    n_y -- scalar, scalar, number of classe (from 0 to 9, -> 10)\n",
    "    \n",
    "    returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    '''\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape = [n_x, None], name = 'X')\n",
    "    Y = tf.placeholder(tf.float32, shape = [n_y, None], name = 'Y')\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    ''' initializes parameters to build a neural network with tensorflow.\n",
    "        (layer_dims: the numbers of layers)\n",
    "    \n",
    "    argument:\n",
    "    weight: W; bias: b\n",
    "    '''\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = tf.get_variable('W'+str(l), [layer_dims[l], layer_dims[l-1]], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        parameters['b' + str(l)] = tf.get_variable('b'+str(l), [layer_dims[l], 1], initializer = tf.zeros_initializer())\n",
    "        print(\"W\"+str(l), parameters['W' + str(l)])\n",
    "        print(\"b\"+str(l), parameters['b' + str(l)])\n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters, p = 1.0):\n",
    "    '''  Implements the forward propagation for the model: \n",
    "    LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    A = X\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_ = A\n",
    "        A = tf.nn.relu(tf.matmul(parameters['W'+ str(l)], A_) + parameters['b'+str(l)])\n",
    "    ZL = tf.matmul(parameters['W'+ str(L)], A) + parameters['b'+str(L)]\n",
    "    \n",
    "    return ZL\n",
    "\n",
    "def fc_layer(n, inputs, input_dims, out_dims, activation = None):\n",
    "    '''全连接层'''\n",
    "    W = tf.get_variable('W' + str(n), [out_dims, input_dims], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.get_variable('b', + str(n), [out_dims, 1], initializer = tf.zeros_initializer())\n",
    "    Z = tf.add(tf.matmul(W, inputs), b)\n",
    "    if activation == None:\n",
    "        outputs = Z\n",
    "    elif activation == 'relu':\n",
    "        outputs = tf.nn.relu(Z)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "# with tf.Session() as sess:\n",
    "#     layer_dims = [2,3,4]\n",
    "#     X,Y = create_placeholders(2, 4)\n",
    "#     parameters = initialize_parameters(layer_dims)\n",
    "#     print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "#     print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "#     print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "#     print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "#     ZL = forward_propagation(X, parameters)\n",
    "#     print(ZL)\n",
    "\n",
    "def compute_cost(ZL, Y):\n",
    "    ''' '''\n",
    "    ZL = tf.transpose(ZL)\n",
    "    Y = tf.transpose(Y)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = ZL, labels = Y))\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def model_train(train_data, train_label, val_data, val_label, layer_dims, learning_rate = 0.01,\n",
    "          num_epochs = 100, minibatch_size = 64, print_cost = True):\n",
    "    \n",
    "    ops.reset_default_graph()\n",
    "    n_x, n_y = train_data.shape[0], train_label.shape[0]\n",
    "    print(n_x,n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    print(X,Y)\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    ZL = forward_propagation(X, parameters)\n",
    "    print(ZL)\n",
    "    cost = compute_cost(ZL, Y)\n",
    "    print(cost)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(ZL, 0), tf.argmax(Y, 0))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_cost = 0\n",
    "            num_minibatches = int(train_data.shape[1] / minibatch_size)\n",
    "#             print(num_minibatches)\n",
    "            minibatches = random_mini_batches(train_data, train_label, minibatch_size)\n",
    "                        \n",
    "            for minibatch in minibatches:\n",
    "                (mini_X, mini_Y) = minibatch  #                 mini_X, mini_Y = mnist.train.next_batch(minibatch_size)#                 tr_X ,Tr_Y= mini_X.T, mini_Y.T\n",
    "#                 print(mini_X.shape, mini_Y.shape)\n",
    "                _, mini_cost = sess.run([optimizer, cost], feed_dict = {X:mini_X, Y:mini_Y})                \n",
    "                epoch_cost += mini_cost / num_minibatches               \n",
    "#             print(mini_cost)\n",
    "#             print(epoch_cost)\n",
    "            \n",
    "            val_loss,acc = sess.run([cost,accuracy], feed_dict = {X:val_dyrrrrrrrrrata, Y:val_label})\n",
    "                \n",
    "            if print_cost == True and (epoch+1) % 10 == 0:\n",
    "                print(\"Train epoch: {}, Train_loss = {:.6f} || Validation_loss = {:.6f}, Accuracy = {:.4f}\".format(epoch+1, epoch_cost, val_loss, acc))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-91cd39387936>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From E:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From E:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting G:/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting G:/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting G:/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting G:/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From E:\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "784 10\n",
      "Tensor(\"X:0\", shape=(784, ?), dtype=float32) Tensor(\"Y:0\", shape=(10, ?), dtype=float32)\n",
      "W1 <tf.Variable 'W1:0' shape=(256, 784) dtype=float32_ref>\n",
      "b1 <tf.Variable 'b1:0' shape=(256, 1) dtype=float32_ref>\n",
      "W2 <tf.Variable 'W2:0' shape=(100, 256) dtype=float32_ref>\n",
      "b2 <tf.Variable 'b2:0' shape=(100, 1) dtype=float32_ref>\n",
      "W3 <tf.Variable 'W3:0' shape=(10, 100) dtype=float32_ref>\n",
      "b3 <tf.Variable 'b3:0' shape=(10, 1) dtype=float32_ref>\n",
      "Tensor(\"add_2:0\", shape=(10, ?), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-3-91cd39387936>:94: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Train epoch: 10, Train_loss = 0.073379 || Validation_loss = 0.122512, Accuracy = 0.9688\n",
      "Train epoch: 20, Train_loss = 0.040594 || Validation_loss = 0.172892, Accuracy = 0.9740\n",
      "Train epoch: 30, Train_loss = 0.046462 || Validation_loss = 0.196910, Accuracy = 0.9724\n",
      "Train epoch: 40, Train_loss = 0.039182 || Validation_loss = 0.231917, Accuracy = 0.9732\n",
      "Train epoch: 50, Train_loss = 0.020206 || Validation_loss = 0.265946, Accuracy = 0.9744\n",
      "Train epoch: 60, Train_loss = 0.033107 || Validation_loss = 0.263454, Accuracy = 0.9752\n",
      "Train epoch: 70, Train_loss = 0.039062 || Validation_loss = 0.399902, Accuracy = 0.9724\n",
      "Train epoch: 80, Train_loss = 0.019706 || Validation_loss = 0.500828, Accuracy = 0.9762\n",
      "Train epoch: 90, Train_loss = 0.023425 || Validation_loss = 0.460694, Accuracy = 0.9766\n",
      "Train epoch: 100, Train_loss = 0.038048 || Validation_loss = 0.572833, Accuracy = 0.9722\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [784,256,100,10]\n",
    "train_data,train_label,val_data,val_label,test_data,test_label = load_data()\n",
    "peremeters = model_train(train_data, train_label, val_data, val_label, layer_dims,minibatch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
