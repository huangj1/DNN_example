{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AlexNet:\n",
    "<img src=\"AlexNet/AlexNet.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_placeholder(x_shape, num_class, name = 'input_data'): #数据占位空间\n",
    "    '''x_shape:输入图像维度（如[224,224,3]）\n",
    "       num_class:输出类别数目（如10）\n",
    "    '''\n",
    "    with tf.name_scope(name):\n",
    "        X = tf.placeholder(tf.float32, [None, x_shape[-3], x_shape[-2], x_shape[-1]], name = 'X')\n",
    "        Y = tf.placeholder(tf.float32, [None, num_class], name = 'Y')\n",
    "        tf.summary.image('X', X, 9) #加入tensorboard\n",
    "    return X, Y\n",
    "\n",
    "def conv(x, filter_h, filter_w, num_filters, stride_h, stride_x, name, padding = 'SAME', trainable = True): #卷积层操作\n",
    "    '''x:输入（[N,H,W,C]）； filter_h, filter_w:卷积核维度； num_filters:卷积核个数；\n",
    "       stride_h, stride_x:卷积滑动步长； padding:填充；name:名称\n",
    "    '''\n",
    "    input_channels = int(x.get_shape().as_list()[-1])\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('weights', shape = [filter_h,filter_w,input_channels,num_filters], initializer = tf.contrib.layers.xaver_initializer(), trainable = trainable)\n",
    "        b = tf.get_variable('biases', shape = [num_filters], initializer = tf.zeros_initializer(), trainable = trainable)\n",
    "        conv = tf.nn.conv2d(x, W, strides=[1,stride_y,stride_x,1], padding = padding)\n",
    "        relu = tf.nn.relu(tf.nn.bias_add(conv, b))\n",
    "        tf.summary.histogram('weights', W)\n",
    "        tf.summary.histogram('biases', b)\n",
    "    return relu\n",
    "\n",
    "def max_pool(x, filter_h, filter_w, stride_h, stride_x, name, padding = 'SAME'): #最大池化\n",
    "    return tf.nn.max_pool(x, ksize = [1, filter_h, filter_w, 1], strides = [1, stride_h, stride_w, 1], padding = padding, name = name)\n",
    "    \n",
    "def fc(x, input_dim, output_dim, name, relu=True, trainable = True): #全连接层\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('weights', shape = [input_dim, output_dim], initializer = tf.contrib.layers.xavier_initializer(), trainable=trainable)\n",
    "        b = tf.get_variable('biases', shape = [output_dim], trainable=trainable)\n",
    "        tf.summary.histogram('weights', W)\n",
    "        tf.summary.histogram('biases', b)\n",
    "        if relu:\n",
    "            return tf.nn.relu(tf.matmal(x, W) + b)\n",
    "        else:\n",
    "            return tf.add(tf.matmul(x, W), b)\n",
    "\n",
    "def lrn(x, radius, alpha, beta, name, bias=1.0): #局部响应归一化\n",
    "    return tf.nn.local_response_normalization(x, depth_radius=radius, alpha=alpha, beta=beta, bias=bias, name=name)\n",
    "\n",
    "def dropout(x, keep_prob, name): #dropout\n",
    "    return tf.nn.dropout(x, keep_prob, name=name)\n",
    "\n",
    "class AlexNet():\n",
    "    def __init__(self, x, y, model_dir,\n",
    "                 batch_size=1, learning_rate=0.01, \n",
    "                 keep_prob=0.5, num_steps=10000):\n",
    "        self.TRAIB_IMAGES = x\n",
    "        self.TRAIN_LABELS = y\n",
    "        self.NUM_CLASS = y.shape[1]\n",
    "        self.BATCH_SIZE = batch_size\n",
    "        self.LEARNING_RATE = learning_rate\n",
    "        self.KEEP_PROB = keep_prob\n",
    "        self.NUM_STEPS = num_steps\n",
    "        self.SAVE_PATH = model_dir\n",
    "        self.X,self.Y = create_placeholder(self.TRAIB_IMAGES, self.NUM_CLASS)\n",
    "#         self.backward()\n",
    "        \n",
    "    def forward(self):\n",
    "        '''Create the network'''\n",
    "\n",
    "        # 1st Layer: Conv(relu) -> Lrn -> Pool\n",
    "        conv1 = conv(self.X, 11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "        norm1 = lrn(conv1, 2, 1e-4, 0.75, name='norm1')\n",
    "        pool1 = max_pool(norm1, 3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "        \n",
    "        # 2nd Layer: Conv(relu) -> Lrn -> Pool\n",
    "        conv2 = conv(pool1, 5, 5, 256, 1, 1, name='conv2')\n",
    "        norm2 = lrn(conv2, 2, 1e-4, 0.75, name='norm2')\n",
    "        pool2 = max_pool(norm2, 3, 3, 2, 2, padding='VALID', name='pool2')\n",
    "        \n",
    "        # 3rd Layer: Conv(relu)\n",
    "        conv3 = conv(pool2, 3, 3, 384, 1, 1, name='conv3')\n",
    "        \n",
    "        # 4th Layer: Conv(relu)\n",
    "        conv4 = conv(conv3, 3, 3, 384, 1, 1, name='conv4')\n",
    "        \n",
    "        # 5th Layer: Conv(relu) -> Pool\n",
    "        conv5 = conv(conv4, 3, 3, 256, 1, 1, name='conv5')\n",
    "        pool5 = max_pool(conv5, 3, 3, 2, 2, padding='VALID', name='pool5')\n",
    "        \n",
    "        # 6th Layer: Flatten -> FC(relu) -> Dropout\n",
    "        flattened = tf.contrib.layers.flatten(pool5)\n",
    "        fc6 = fc(flattened, flattened.get_shape().as_list()[-1], 4096, name='fc6')\n",
    "        dropout6 = dropout(fc6, self.KEEP_PROB)\n",
    "        \n",
    "        # 7th Layer: FC(relu) -> Dropout\n",
    "        fc7 = fc(dropout6, 4096, 4096, name='fc7')\n",
    "        dropout7 = dropout(fc7, self.KEEP_PROB)\n",
    "        \n",
    "        # 7th Layer: FC(relu=False)\n",
    "        fc8 = fc(fc7, 4096, self.NUM_CLASS, relu=False, name='fc8')\n",
    "        \n",
    "        return fc8\n",
    "    \n",
    "    def backward(self):\n",
    "        num_epoch = 100\n",
    "        num_batches = int(num_examples / self.BATCH_SIZE)\n",
    "        \n",
    "        Z = self.forward()\n",
    "        global_step = tf.Variable(0, trainable=False, name='num_steps')\n",
    "        tf.summary.scalar('learning_rate', self.LEARNING_RATE)\n",
    "        with tf.name_scope('Optimizer'):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.Z, labels=self.Y))\n",
    "            train_step = tf.train.GradientDescentOptimizer(self.LEARNING_RATE).minimize(loss, global_step=global_step)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(Z,1), tf.argmax(self.Y,1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "            tf.summary.scalar('train', accuracy)\n",
    "            \n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session(config=config) as sess:\n",
    "            merged = tf.summary.merge_all()\n",
    "            writer = tf.summary.FileWriter(SAVE_PATH, sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            ckpt = tf.train.get_checkpoint_state(SAVE_PATH)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                \n",
    "            for epoch in range(num_epoch):\n",
    "                for num_batch in range(num_batches):\n",
    "                    _, loss_value, acc, summary, step = sess.run([train_step, loss, accuracy, merged, global_step], \n",
    "                                                                 feed_dict = {self.X:batch_images, self.Y:batch_labels})\n",
    "                    writer.add_summary(summary, step)\n",
    "                    print('INFO: After %d training iteration(s), loss is %.5f, batch_train accuracy is %.4f' % (step, loss_value, acc))\n",
    "                    if step % 100 ==0:\n",
    "                        saver.save(sess, self.SAVE_PATH+'AlexNet_Model.ckpt', global_step = global_step)\n",
    "                        print('AlexNet_Model.ckpt-%d already saved!' % step)\n",
    "                    if step >= NUM_STEPS: return\n",
    "#                 saver.save(sess, self.SAVE_PATH+'AlexNet_Model.ckpt-%d'% (epoch+1)*num_batches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train.py  (x, y, model_dir, batch_size=1, learning_rate=0.01, keep_prob=0.5, num_steps=10000)\n",
    "Example usage:\n",
    "    ./train.py --model_dir='./path/' \\\n",
    "        --num_steps=10000 \\\n",
    "\n",
    "'''\n",
    "# import tensorflow as tf\n",
    "# import os\n",
    "\n",
    "# TRAIN_IMAGES = \n",
    "# TRAIN_LABELS = \n",
    "# BATCH_SIZE = 256\n",
    "# # NUM_STEPS = 10000\n",
    "# # MODEL_DIR\n",
    "\n",
    "# flags = tf.app.flags\n",
    "# flags.DEFINE_string('model_dir', None, \n",
    "#                     'Path to output model directory where event and checkpoint files will be written.')\n",
    "# flags.DEFINE_integer('num_steps', None, 'Number of train steps.')\n",
    "\n",
    "# FLAGS = flags.FLAGS\n",
    "# MODEL_DIR = FLAGS.model_dir\n",
    "# NUM_STEPS = FLAGS.num_steps\n",
    "# if not os.path.exists(MODEL_DIR): os.mkdir(MODEL_DIR)\n",
    "\n",
    "# model = AlexNet(TRAIN_IMAGES, TRAIN_LABELS, MODEL_DIR, BATCH_SIZE, NUM_STEPS)\n",
    "# model.backward()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IMAGES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-af24859ac64b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# LABELS =\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# MODEL_DIR =\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAlexNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIMAGES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLABELS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMODEL_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IMAGES' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "valuation.py  (x, y, model_dir, batch_size, learning_rate=0.01, keep_prob=0.5, num_steps)\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "# IMAGES = \n",
    "# LABELS = \n",
    "# MODEL_DIR = \n",
    "model = AlexNet(IMAGES, LABELS, MODEL_DIR, keep_prob=1)\n",
    "x = model.X\n",
    "y = model.Y\n",
    "y_ = model.forward()\n",
    "saver = tf.train.Saver()\n",
    "correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(MODEL_DIR)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        steps = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "        acc = sess.run(accuracy, feed_dict = {x:IMAGES, y:LABELS})\n",
    "        print('After %s training step(s), evaluation accuracy is %.4f' % (steps, acc))\n",
    "    else:\n",
    "        print('No checkpoint file found!')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
